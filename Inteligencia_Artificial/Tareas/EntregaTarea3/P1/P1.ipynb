{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ Pregunta \\ 1 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a)$ Primero que todo definiremos que es el sesgo inductivo y que es sesgo en la predicción.\n",
    "\n",
    "$Sesgo \\ inductivo:$ Es el conjunto de suposiciones que se utilizan para predecir soluciones a un problema especifico. Por ejemplo, un sesgo inductivo podría ser asumir que la solución debe ser una regresion lineal o polinomica.\n",
    "\n",
    "$Sesgo \\ en \\ la \\ predicción:$ Corresponde al error que se comete en el output de un algoritmo. Viene dado por la formula:\n",
    "\n",
    "$ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ [E[\\hat{f}(x)] - f(x)]^2 $\n",
    "\n",
    "$ \\ $ \n",
    "\n",
    "De esta manera, podriamos decir que ambas definiciones son muy distintas, sin embargo, es posible establecer una relación entre ambas, pues es mi sesgo inductivo el que define mi $f$ a utilizar y como este $f$ influye en el sesgo de predicción, entonces podemos decir que el sesgo en la predicción se ve fuertemente influido por mi sesgo inductivo.  \n",
    "\n",
    "Para verlo más en simple pondremos un ejemplo. Supongamos que tengo datos que tienen una forma muy parecida a una recta y pongamosnos los siguientes 2 casos:\n",
    "\n",
    "i) Mi sesgo inductivo es el de un regresión lineal. Mi error en la predicción sería pequeño. \n",
    "\n",
    "ii) Mi sesgo inductivo es el de una regresión formada por una suma de senos y cosenos. Mi error en la predicción sería grande. \n",
    "\n",
    "Así de esta manera podemos ejemplificar claramente la relación que existe entre ambos sesgos. \n",
    "\n",
    "$  $ \n",
    "\n",
    "$  $ \n",
    "\n",
    "$  $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$b)$ Los sesgos inductivos para k-NN y para los arboles de decisión son los siguientes:\n",
    "\n",
    "$ k-NN:$ El sesgo inductivo de este algoritmo es que los datos estan dividos en grupos de caracteristicas similares, es por esto que en este algoritmo lo que se hace para predecir un dato, es que dadas sus caracteristicas (o features) se observan los k vecinos más cercanos y entre estos vecinos se ve el grupo que más se repite y ese grupo es el que se le asigna al dato. \n",
    "\n",
    "$ Arboles \\ basados \\ en \\ ganancia \\ de \\ informacion: $ El sesgo inductivo de este algoritmo es generar arboles que sean más anchos que profundos, pues este algoritmo genera este tipo de arboles. \n",
    "\n",
    "La relación entre el sesgo de k-NN y la clasificación realizada en un nodo hoja de un arbol es que en ambos casos los que se hace es analizar sobre un grupo de opciones a cual es el que se parece más y partir de eso, clasificarlo como ese \"grupo más similar\". \n",
    "\n",
    "\n",
    "$ \\ $ \n",
    "\n",
    "$ \\ $\n",
    "\n",
    "$ \\ $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ c) $ La base de datos que vamos a utilizar en esta pregunta es \"2014_Financial_Data.csv\" es decir, la base de datos que\n",
    "nos dieron por enunciado (archivo que adjuntamos junto a esta pregunta).\n",
    "\n",
    "Se implementaron dos arboles de decisión. Uno basado en la mínima ganancia de información y otro basado en la máxima ganancia de información, en donde más adelante al obtener resultados se realiza su analisis respectivo. \n",
    "\n",
    "Pero antes de implementar estos arboles debemos decir que como estamos trabajando con variables númericas a cada columna de información se le tiene que realizar el siguiente preprocesamiento. \n",
    "\n",
    "Se debe elegir un $n$ tal que al separar una columna en los datos que son menor o igual $n$ y en los que son mayor que $n$ nos genere la máxima ganancia de información posible. Para poder determinar este $n$ lo que hicimos fue que en cada columna consideramos como candidatos a los mismos valores que conformaban a la columna. Una vez realizado esto, procedemos a aplicar el algoritmo de minima/maxima información.  Esto se debe realizar para cada split.\n",
    "\n",
    "$ \\ $\n",
    "\n",
    "$ \\ $\n",
    "\n",
    "$ \\ $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def son_todos_iguales(clase):\n",
    "    if len(clase) == 1:\n",
    "        return True\n",
    "    primer_elemento = clase[0]\n",
    "    for i in range(len(clase)-1):\n",
    "        if clase[i+1] != primer_elemento:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_separador(columna, clase):\n",
    "    \n",
    "    lista_entropia = []\n",
    "    lista_valores = []\n",
    "    \n",
    "    cantidad_positivos_padre = 0\n",
    "    cantidad_negativos_padre = 0\n",
    "\n",
    "    for i in range(len(columna)):\n",
    "        if clase[i] == 1:\n",
    "            cantidad_positivos_padre = cantidad_positivos_padre + 1\n",
    "        else:\n",
    "            cantidad_negativos_padre = cantidad_negativos_padre + 1\n",
    "\n",
    "    p_positivos_padre = cantidad_positivos_padre/(cantidad_positivos_padre+cantidad_negativos_padre)\n",
    "    p_negativos_padre = cantidad_negativos_padre/(cantidad_positivos_padre+cantidad_negativos_padre)\n",
    "\n",
    "    entropia_padre = 0\n",
    "\n",
    "    if p_positivos_padre != 0 and p_negativos_padre != 0:\n",
    "        entropia_padre = entropia_padre - p_positivos_padre*math.log2(p_positivos_padre)\n",
    "        entropia_padre = entropia_padre - p_negativos_padre*math.log2(p_negativos_padre)\n",
    "    \n",
    "    # CALCULO DE ENTROPIA PARA HIJOS\n",
    "    \n",
    "    for i in range(len(columna)):\n",
    "        \n",
    "        lista_menor = []\n",
    "        lista_mayor = []\n",
    "        \n",
    "        for j in range(len(columna)):\n",
    "            if columna[j] <= columna[i]:\n",
    "                lista_menor.append([columna[j], j])\n",
    "            else:\n",
    "                lista_mayor.append([columna[j], j])\n",
    "                \n",
    "        \n",
    "        p_hijo_mayor = len(lista_mayor)/(len(lista_mayor)+len(lista_menor))\n",
    "        p_hijo_menor = len(lista_menor)/(len(lista_mayor)+len(lista_menor))\n",
    "        \n",
    "        # Entropia hijo menor\n",
    "        \n",
    "        cantidad_positivos_hijo_menor = 0\n",
    "        cantidad_negativos_hijo_menor = 0\n",
    "        \n",
    "        for h1 in lista_menor:\n",
    "            if clase[h1[1]] == 1:\n",
    "                cantidad_positivos_hijo_menor = cantidad_positivos_hijo_menor + 1\n",
    "            else:\n",
    "                cantidad_negativos_hijo_menor = cantidad_negativos_hijo_menor + 1\n",
    "        \n",
    "        if cantidad_positivos_hijo_menor != 0 and cantidad_negativos_hijo_menor != 0:\n",
    "            p_positivos_hijo_menor = cantidad_positivos_hijo_menor/(cantidad_positivos_hijo_menor + cantidad_negativos_hijo_menor)\n",
    "            p_negativos_hijo_menor = cantidad_negativos_hijo_menor/(cantidad_positivos_hijo_menor + cantidad_negativos_hijo_menor)\n",
    "        else:\n",
    "            p_positivos_hijo_menor = 0\n",
    "            p_negativos_hijo_menor = 0\n",
    "            \n",
    "        entropia_hijo_menor = 0 \n",
    "        \n",
    "        if p_positivos_hijo_menor != 0 and p_negativos_hijo_menor != 0:\n",
    "            \n",
    "            entropia_hijo_menor = entropia_hijo_menor - p_positivos_hijo_menor*math.log2(p_positivos_hijo_menor)\n",
    "            entropia_hijo_menor = entropia_hijo_menor - p_negativos_hijo_menor*math.log2(p_negativos_hijo_menor)\n",
    "        \n",
    "        # Entropia_hijo_mayor \n",
    "        \n",
    "        cantidad_positivos_hijo_mayor = 0\n",
    "        cantidad_negativos_hijo_mayor = 0\n",
    "        \n",
    "        for h2 in lista_mayor:\n",
    "            if clase[h2[1]] == 1:\n",
    "                cantidad_positivos_hijo_mayor = cantidad_positivos_hijo_mayor + 1\n",
    "            else:\n",
    "                cantidad_negativos_hijo_mayor = cantidad_negativos_hijo_mayor + 1\n",
    "        \n",
    "        if cantidad_positivos_hijo_mayor != 0 and cantidad_negativos_hijo_mayor != 0:\n",
    "            p_positivos_hijo_mayor = cantidad_positivos_hijo_mayor/(cantidad_positivos_hijo_mayor + cantidad_negativos_hijo_mayor)\n",
    "            p_negativos_hijo_mayor = cantidad_negativos_hijo_mayor/(cantidad_positivos_hijo_mayor + cantidad_negativos_hijo_mayor)\n",
    "        else:\n",
    "            p_positivos_hijo_mayor = 0\n",
    "            p_negativos_hijo_mayor = 0\n",
    "            \n",
    "        entropia_hijo_mayor = 0\n",
    "        \n",
    "        if p_positivos_hijo_mayor != 0 and p_negativos_hijo_mayor != 0:\n",
    "            entropia_hijo_mayor = entropia_hijo_mayor  - p_positivos_hijo_mayor*math.log2(p_positivos_hijo_mayor)\n",
    "            entropia_hijo_mayor = entropia_hijo_mayor  - p_negativos_hijo_mayor*math.log2(p_negativos_hijo_mayor)\n",
    "        \n",
    "        entropia_total = entropia_padre - p_hijo_mayor*entropia_hijo_mayor - p_hijo_menor*entropia_hijo_menor\n",
    "        \n",
    "        \n",
    "        lista_entropia.append(entropia_total)\n",
    "        lista_valores.append(columna[i])\n",
    "        \n",
    "    return [lista_entropia, lista_valores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_arbol_maximo(columnas, clase, profundidad, info):\n",
    "    \n",
    "    if son_todos_iguales(clase) == True:\n",
    "        profundidad_maximo.append(profundidad)\n",
    "        return [clase[0], profundidad]\n",
    "    \n",
    "    lista_entropias = []\n",
    "    lista_separadores = []\n",
    "    \n",
    "    for i in range(len(columnas)):\n",
    "        output = calcular_separador(columnas[i], clase)\n",
    "        max_ganancia = max(output[0])\n",
    "        indice = output[0].index(max_ganancia)\n",
    "        separador = output[1][indice]\n",
    "        \n",
    "        lista_entropias.append(max_ganancia)\n",
    "        lista_separadores.append(separador)\n",
    "    \n",
    "    indice_elegido = lista_entropias.index(max(lista_entropias))\n",
    "    \n",
    "    if info[0] == indice_elegido and info[1] == lista_separadores[indice_elegido]:\n",
    "        if np.mean(clase) >= 0.5:\n",
    "            return [1, profundidad]\n",
    "        else:\n",
    "            return [0, profundidad]\n",
    "    \n",
    "    \n",
    "    nuevas_columnas_1 = []\n",
    "    nuevas_columnas_2 = []\n",
    "    \n",
    "    nueva_clase_1 = []\n",
    "    nueva_clase_2 = []\n",
    "    \n",
    "    indices_1 = []\n",
    "    indices_2 = []\n",
    "    \n",
    "    for j in range(len(columnas[indice_elegido])):\n",
    "        if columnas[indice_elegido][j] <= lista_separadores[indice_elegido]:\n",
    "            nueva_clase_1.append(clase[j])\n",
    "            indices_1.append(j)\n",
    "        else:\n",
    "            nueva_clase_2.append(clase[j])\n",
    "            indices_2.append(j)\n",
    "    \n",
    "    for i in range(len(columnas)):\n",
    "        lista_1 = []\n",
    "        for k1 in indices_1:\n",
    "            lista_1.append(columnas[i][k1])\n",
    "        lista_2 = []\n",
    "        for k2 in indices_2:\n",
    "            lista_2.append(columnas[i][k2])\n",
    "        nuevas_columnas_1.append(lista_1)\n",
    "        nuevas_columnas_2.append(lista_2)\n",
    "\n",
    "    info_nueva = [indice_elegido, lista_separadores[indice_elegido]]\n",
    "\n",
    "    if info[0] == indice_elegido and info[1] == lista_separadores[indice_elegido]:\n",
    "        if np.mean(clase) >= 0.5:\n",
    "            return [1, profundidad]\n",
    "        else:\n",
    "            return [0, profundidad]\n",
    "    else:\n",
    "        if len(nueva_clase_1) == 0:\n",
    "            if np.mean(nueva_clase_1) > 0.5:\n",
    "                return [1, profundidad]\n",
    "            else:\n",
    "                return [0, profundidad]\n",
    "        elif len(nueva_clase_2) == 0:\n",
    "            if np.mean(nueva_clase_2) > 0.5:\n",
    "                return [1, profundidad]\n",
    "            else:\n",
    "                return [0, profundidad]\n",
    "        else:\n",
    "            return [calcular_arbol_maximo(nuevas_columnas_1, nueva_clase_1, profundidad + 1, info_nueva),\n",
    "            calcular_arbol_maximo(nuevas_columnas_2, nueva_clase_2, profundidad + 1, info_nueva),\n",
    "            info_nueva]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_arbol_minimo(columnas, clase, profundidad, info):\n",
    "    \n",
    "    if son_todos_iguales(clase) == True:\n",
    "        profundidad_minimo.append(profundidad)\n",
    "        return [clase[0], profundidad]\n",
    "    \n",
    "    lista_entropias = []\n",
    "    lista_separadores = []\n",
    "    \n",
    "    for i in range(len(columnas)):\n",
    "        output = calcular_separador(columnas[i], clase)\n",
    "        max_ganancia = max(output[0])\n",
    "        indice = output[0].index(max_ganancia)\n",
    "        separador = output[1][indice]\n",
    "        \n",
    "        lista_entropias.append(max_ganancia)\n",
    "        lista_separadores.append(separador)\n",
    "    \n",
    "    indice_elegido = lista_entropias.index(min(lista_entropias))\n",
    "    \n",
    "    nuevas_columnas_1 = []\n",
    "    nuevas_columnas_2 = []\n",
    "    \n",
    "    nueva_clase_1 = []\n",
    "    nueva_clase_2 = []\n",
    "    \n",
    "    indices_1 = []\n",
    "    indices_2 = []\n",
    "    \n",
    "    for j in range(len(columnas[indice_elegido])):\n",
    "        if columnas[indice_elegido][j] <= lista_separadores[indice_elegido]:\n",
    "            nueva_clase_1.append(clase[j])\n",
    "            indices_1.append(j)\n",
    "        else:\n",
    "            nueva_clase_2.append(clase[j])\n",
    "            indices_2.append(j)\n",
    "    \n",
    "    for i in range(len(columnas)):\n",
    "        lista_1 = []\n",
    "        for k1 in indices_1:\n",
    "            lista_1.append(columnas[i][k1])\n",
    "        lista_2 = []\n",
    "        for k2 in indices_2:\n",
    "            lista_2.append(columnas[i][k2])\n",
    "        nuevas_columnas_1.append(lista_1)\n",
    "        nuevas_columnas_2.append(lista_2)\n",
    "\n",
    "    info_nueva = [indice_elegido, lista_separadores[indice_elegido]]\n",
    "    \n",
    "    if info[0] == indice_elegido and info[1] == lista_separadores[indice_elegido]:\n",
    "        if np.mean(clase) >= 0.5:\n",
    "            return [1, profundidad]\n",
    "        else:\n",
    "            return [0, profundidad]\n",
    "    else:\n",
    "        if len(nueva_clase_1) == 0:\n",
    "            if np.mean(nueva_clase_1) > 0.5:\n",
    "                return [1, profundidad]\n",
    "            else:\n",
    "                return [0, profundidad]\n",
    "        elif len(nueva_clase_2) == 0:\n",
    "            if np.mean(nueva_clase_2) > 0.5:\n",
    "                return [1, profundidad]\n",
    "            else:\n",
    "                return [0, profundidad]\n",
    "        else:\n",
    "            return [calcular_arbol_minimo(nuevas_columnas_1, nueva_clase_1, profundidad + 1, info_nueva),\n",
    "                    calcular_arbol_minimo(nuevas_columnas_2, nueva_clase_2, profundidad + 1, info_nueva),\n",
    "                    info_nueva]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_valor(indice, resultado, columnas):\n",
    "    if len(resultado) <= 2:\n",
    "        return resultado[0]\n",
    "    else:\n",
    "        seleccion = resultado[2][0]\n",
    "        \n",
    "        if columnas[seleccion][indice] <= resultado[2][1]:\n",
    "            return calcular_valor(indice, resultado[0], columnas)\n",
    "        else:\n",
    "            return calcular_valor(indice, resultado[1], columnas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo = pd.read_csv(\"2014_Financial_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombres = []\n",
    "parte_columnas = []\n",
    "columnas_testeo = []\n",
    "columnas_entrenamiento = []\n",
    "\n",
    "for row in archivo:\n",
    "    nombres.append(row)\n",
    "\n",
    "# Eliminamos el primer elemento\n",
    "\n",
    "nombres = nombres[1:len(nombres)]\n",
    "\n",
    "N = 350\n",
    "M = 10\n",
    "valor_inicial = 0\n",
    "valor_final = valor_inicial + M\n",
    "\n",
    "nombres = nombres[valor_inicial:valor_final]\n",
    "\n",
    "# Eliminamos los datos nan\n",
    "\n",
    "nuevo_archivo = archivo.dropna(subset=nombres)\n",
    "\n",
    "for k in nombres:\n",
    "    parte_columnas.append(nuevo_archivo[k])\n",
    "\n",
    "parte_clases = nuevo_archivo['Class']\n",
    "\n",
    "for i in range(len(parte_columnas)):\n",
    "    lista = []\n",
    "    for j in parte_columnas[i]:\n",
    "        lista.append(j)\n",
    "    columnas_entrenamiento.append(lista[0:N])\n",
    "    columnas_testeo.append(lista[N:len(lista)])\n",
    "\n",
    "clases_totales = [] \n",
    "\n",
    "for i in parte_clases:\n",
    "    clases_totales.append(i)\n",
    "    \n",
    "clase_entrenamiento = clases_totales[0:N]\n",
    "clase_testeo = clases_totales[N:len(clases_totales)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "profundidad_maximo = []\n",
    "profundidad_minimo = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_maximo = calcular_arbol_maximo(columnas_entrenamiento, clase_entrenamiento, 0, ['A', 'A']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El porcentaje de acierto en el testeo es:  55.0450170754424\n",
      "La profundidad maxima del arbol es: 15\n",
      "La cantidad de hojas del arbol es: 69\n"
     ]
    }
   ],
   "source": [
    "# Aciertos en testeo\n",
    "\n",
    "lista_predicciones = []\n",
    "\n",
    "for j in range(len(clase_testeo)):\n",
    "    lista_predicciones.append(calcular_valor(j, output_maximo, columnas_testeo))\n",
    "    \n",
    "lista_coincidencias = []\n",
    "\n",
    "for j in range(len(clase_testeo)):\n",
    "    if lista_predicciones[j] == clase_testeo[j]:\n",
    "        lista_coincidencias.append(1)\n",
    "    else:\n",
    "        lista_coincidencias.append(0)\n",
    "\n",
    "print(\"El porcentaje de acierto en el testeo es: \", np.mean(lista_coincidencias)*100)\n",
    "print(\"La profundidad maxima del arbol es:\", max(profundidad_maximo))\n",
    "print(\"La cantidad de hojas del arbol es:\", len(profundidad_maximo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\56977\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\56977\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "output_minimo = calcular_arbol_minimo(columnas_entrenamiento, clase_entrenamiento, 0, ['A', 'A']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El porcentaje de acierto en el testeo es:  51.87829866501087\n",
      "La profundidad maxima del arbol es: 38\n",
      "La cantidad de hojas del arbol es: 91\n"
     ]
    }
   ],
   "source": [
    "# Aciertos en testeo\n",
    "\n",
    "lista_predicciones = []\n",
    "\n",
    "for j in range(len(clase_testeo)):\n",
    "    lista_predicciones.append(calcular_valor(j, output_minimo, columnas_testeo))\n",
    "    \n",
    "lista_coincidencias = []\n",
    "\n",
    "for j in range(len(clase_testeo)):\n",
    "    if lista_predicciones[j] == clase_testeo[j]:\n",
    "        lista_coincidencias.append(1)\n",
    "    else:\n",
    "        lista_coincidencias.append(0)\n",
    "\n",
    "print(\"El porcentaje de acierto en el testeo es: \", np.mean(lista_coincidencias)*100)\n",
    "print(\"La profundidad maxima del arbol es:\", max(profundidad_minimo))\n",
    "print(\"La cantidad de hojas del arbol es:\", len(profundidad_minimo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ $\n",
    "\n",
    "$ $\n",
    "\n",
    "$ Analisis \\ de \\ los \\ resultados: $ \n",
    "\n",
    "Como se observa para un arbol basado en la máxima ganancia de información se obtiene un arbol de profundidad baja (15) y con muchas hojas (69) en relación a su profundidad (69/15 = 4,6) y en el arbol basado en la minima ganancia de información se obtiene un arbol de profundidad alta (38) y con una cantidad de hojas (91) que resulta en un proporcion hojas/profundidad (91/38 = 2,4) menor que la de un arbol basado en la maxima ganancia de información.\n",
    "\n",
    "Así se corrobora el hecho de que el sesgo inductivo de los arboles basados en minima ganancia de información es generar arboles profundos y que el sesgo inductivo de los arboles basados en la maxima ganancia de información generan arboles poco profundos y anchos.\n",
    "\n",
    "Ahora, dado a que el porcentaje de acierto en testeo para el arbol basado en la minima ganancia de información es de $51.88$ % y el porcentaje de acierto en testeo para el arbol basado en la máxima ganancia de información es de $55.05$ % entonces decimos que el nuevo sesgo inductivo NO es mejor que el de un arbol tradicional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
